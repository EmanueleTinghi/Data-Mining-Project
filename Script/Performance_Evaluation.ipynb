{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import _tree\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import LeaveOneGroupOut, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer, confusion_matrix\n",
    "from scipy.stats import wilcoxon\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__UTILITY FUNCTIONS__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts tree in a readable list of rules \n",
    "def tree_to_code(tree, feature_names):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != _tree.TREE_UNDEFINED else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "    print(\"def tree({}):\".format(\", \".join(feature_names)))\n",
    "\n",
    "    def recurse(node, depth):\n",
    "        indent = \"  \" * depth\n",
    "        if tree_.feature[node] != _tree.TREE_UNDEFINED:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            print(\"{}if {} <= {}:\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_left[node], depth + 1)\n",
    "            print(\"{}elif {} > {}\".format(indent, name, threshold))\n",
    "            recurse(tree_.children_right[node], depth + 1)\n",
    "        else:\n",
    "            print(\"{}return {}\".format(indent, np.argmax(tree_.value[node])))\n",
    "\n",
    "    recurse(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_plot(df, var1, var2, tester):\n",
    "    x = df[var1]\n",
    "    y = df[var2]\n",
    "    classes = df['Class']\n",
    "\n",
    "    map = {0.0: 'Left', 1.0: 'Centered', 2.0: 'Right'}\n",
    "\n",
    "    # Replace values in the array\n",
    "    string_array = np.where(np.isin(classes, list(map.keys())), [map[val] for val in classes], classes)\n",
    "\n",
    "    unique = list(set(string_array))\n",
    "    \n",
    "    # Assign colors based on unique values\n",
    "    colors = plt.cm.get_cmap('jet', len(unique))\n",
    "\n",
    "    for i, u in enumerate(unique):\n",
    "        xi = [x[j] for j in range(len(x)) if string_array[j] == u]\n",
    "        yi = [y[j] for j in range(len(x)) if string_array[j] == u]\n",
    "        plt.scatter(xi, yi, c=[colors(i)], label=str(u))\n",
    "\n",
    "    plt.xlabel(var1)  # Fix: 'set_xlabel' should be 'xlabel'\n",
    "    plt.ylabel(var2)  # Fix: 'set_ylabel' should be 'ylabel'\n",
    "\n",
    "    plt.title(str(tester))\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__DATA LOADING__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lorenzo = pd.read_csv(\"./../Dati_unificati/lorenzo_smooth.csv\")\n",
    "Leo = pd.read_csv(\"./../Dati_unificati/leo_smooth.csv\")\n",
    "Irene = pd.read_csv(\"./../Dati_unificati/irene_smooth.csv\")\n",
    "Carlotta = pd.read_csv(\"./../Dati_unificati/carlotta_smooth.csv\")\n",
    "\n",
    "ds = pd.concat([Lorenzo, Leo, Irene, Carlotta], ignore_index=True)\n",
    "\n",
    "X = ds.drop(columns=['Class', 'Tester'])\n",
    "y = ds['Class']\n",
    "groups = ds['Tester']\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(groups.shape)\n",
    "\n",
    "print(groups.drop_duplicates())\n",
    "\n",
    "loso_cv = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__KNN__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_KNN = []\n",
    "\n",
    "#gives 800 values of test_fscore to be checked with Wilcoxon signed rank test\n",
    "for i in range(1, 201):    \n",
    "    classifier_KNN = KNeighborsClassifier(n_neighbors=6)\n",
    "    scaler = RobustScaler()\n",
    "    feat_sel = SelectKBest(k=2)\n",
    "\n",
    "    estimators_KNN = [('scaling', scaler), ('feature-selection', feat_sel), ('clf', classifier_KNN)]\n",
    "    pipe_KNN = Pipeline(estimators_KNN)\n",
    "\n",
    "    scores_KNN = cross_validate(pipe_KNN,\n",
    "                                X,\n",
    "                                y,\n",
    "                                return_estimator=True,\n",
    "                                cv=loso_cv,\n",
    "                                n_jobs=-1,\n",
    "                                groups=groups,\n",
    "                                error_score=\"raise\",\n",
    "                                scoring={'fscore': make_scorer(f1_score, average='weighted'),\n",
    "                                            'accuracy': make_scorer(accuracy_score)}\n",
    "                                )\n",
    "    results_KNN.append(pd.DataFrame(scores_KNN))\n",
    "\n",
    "# Concatenate results outside the loop\n",
    "final_results_KNN = pd.concat(results_KNN, ignore_index=True)\n",
    "\n",
    "print(final_results_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__RANDOM FOREST__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_RF = []  # List to store results\n",
    "\n",
    "#gives 800 values of test_fscore to be checked with Wilcoxon signed rank test\n",
    "for i in range(1, 201):\n",
    "    classifier_RF = RandomForestClassifier(n_estimators=61, min_samples_split=30, min_samples_leaf=1, max_features='sqrt', bootstrap=True, max_depth=10, random_state=i)\n",
    "    scaler = RobustScaler()\n",
    "    feat_sel = SelectKBest(k=2)\n",
    "    estimators_RF = [('scaling', scaler), ('feature-selection', feat_sel), ('clf', classifier_RF)]\n",
    "    pipe_RF = Pipeline(estimators_RF)\n",
    "\n",
    "    scores_RF = cross_validate(pipe_RF,\n",
    "                               X,\n",
    "                               y,\n",
    "                               return_estimator=True,\n",
    "                               cv=loso_cv,\n",
    "                               n_jobs=-1,\n",
    "                               groups=groups,\n",
    "                               error_score=\"raise\",\n",
    "                               scoring={'fscore': make_scorer(f1_score, average='weighted'),\n",
    "                                        'accuracy': make_scorer(accuracy_score)}\n",
    "                               )\n",
    "    \n",
    "    results_RF.append(pd.DataFrame(scores_RF))\n",
    "\n",
    "# Concatenate results outside the loop\n",
    "final_results_RF = pd.concat(results_RF, ignore_index=True)\n",
    "\n",
    "print(final_results_RF)\n",
    "print(final_results_RF.iloc[final_results_RF['test_fscore'].idxmin()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__ADABOOST__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_AB = []\n",
    "\n",
    "#gives 800 values of test_fscore to be checked with Wilcoxon signed rank test\n",
    "for i in range(1,201):\n",
    "    classifier_AdaBoost = AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=61, min_samples_split=30, min_samples_leaf=1, max_features='sqrt', bootstrap=True, max_depth=10, random_state=i), n_estimators=83, random_state=i)\n",
    "    scaler = RobustScaler()\n",
    "    feat_sel = SelectKBest(k=2)\n",
    "\n",
    "    estimators_AdaBoost = [('scaling', scaler), ('feature-selection', feat_sel), ('clf', classifier_AdaBoost)]\n",
    "    pipe_AdaBoost = Pipeline(estimators_AdaBoost)\n",
    "\n",
    "    scores_AdaBoost = cross_validate(pipe_AdaBoost,\n",
    "                            X,\n",
    "                            y,\n",
    "                            return_estimator = True,\n",
    "                            cv = loso_cv,\n",
    "                            n_jobs=-1,\n",
    "                            groups=groups,\n",
    "                            error_score=\"raise\",\n",
    "                            scoring = {'fscore': make_scorer(f1_score, average='weighted'),\n",
    "                                        'accuracy': make_scorer(accuracy_score)}\n",
    "    )\n",
    "    results_AB.append(pd.DataFrame(scores_AdaBoost))\n",
    "    \n",
    "# Concatenate results outside the loop\n",
    "final_results_AdaBoost = pd.concat(results_AB, ignore_index=True)\n",
    "print(final_results_AdaBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__PERFORMANCE EVALUATION__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({'KNN': final_results_KNN['test_fscore'],\n",
    "                        'RF': final_results_RF['test_fscore'],\n",
    "                        'AB': final_results_AdaBoost['test_fscore']})\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot of the results of the three models\n",
    "ax = metrics.boxplot(figsize = (3,3))\n",
    "ax.set_ylabel('f-score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wilcoxon(metrics.KNN, metrics.RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wilcoxon(metrics.KNN, metrics.AB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wilcoxon(metrics.RF, metrics.AB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean of RF: \" + str(np.mean(metrics.RF)))\n",
    "print(\"Mean of AB: \" + str(np.mean(metrics.AB)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "__TREE ANALYSIS OF ADABOOST__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a classifier that tries to predict the tester with the worst performances to see why\n",
    "\n",
    "classifier_AdaBoost = AdaBoostClassifier(estimator=RandomForestClassifier(n_estimators=61, min_samples_split=30, min_samples_leaf=1, max_features='sqrt', bootstrap=True, max_depth=10), n_estimators=83, random_state=0)\n",
    "scaler = RobustScaler()\n",
    "feat_sel = SelectKBest(k=2)\n",
    "\n",
    "estimators_AdaBoost = [('scaling', scaler), ('feature-selection', feat_sel), ('clf', classifier_AdaBoost)]\n",
    "pipe_AdaBoost_tree = Pipeline(estimators_AdaBoost)\n",
    "\n",
    "X_train = ds[(ds['Tester'] != 'Lorenzo')].drop(columns=['Class', 'Tester'])\n",
    "X_test= ds[(ds['Tester'] == 'Lorenzo')].drop(columns=['Class', 'Tester'])\n",
    "\n",
    "y_train = ds[(ds['Tester'] != 'Lorenzo')]['Class']\n",
    "y_test= ds[(ds['Tester'] == 'Lorenzo')]['Class']\n",
    "\n",
    "pipe_AdaBoost_tree.fit(X_train, y_train)\n",
    "y_pred = pipe_AdaBoost_tree.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "\n",
    "#the selected k=2 best feature chosen by the model\n",
    "print(X.columns[pipe_AdaBoost_tree['feature-selection'].get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visulization of the tree rules\n",
    "tree_to_code(pipe_AdaBoost_tree['clf'].estimators_[0].estimators_[31], X.columns[pipe_AdaBoost_tree['feature-selection'].get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit_transform(X_test[[X.columns[pipe_AdaBoost_tree['feature-selection'].get_support()][0], X.columns[pipe_AdaBoost_tree['feature-selection'].get_support()][1]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = X.columns[pipe_AdaBoost_tree['feature-selection'].get_support()][0]\n",
    "feature2 = X.columns[pipe_AdaBoost_tree['feature-selection'].get_support()][1]\n",
    "print(feature1)\n",
    "print(feature2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = X.columns[pipe_AdaBoost_tree['feature-selection'].get_support()][0]\n",
    "feature2 = X.columns[pipe_AdaBoost_tree['feature-selection'].get_support()][1]\n",
    "tree_test = X_test[[feature1, feature2]]\n",
    "tree_test_scaled = pd.DataFrame(scaler.fit_transform(tree_test))\n",
    "print(tree_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree = pipe_AdaBoost_tree['clf'].estimators_[0].estimators_[31].predict(tree_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the lorenzo dataset to confront it agaist the tree rules\n",
    "columns = ds[(ds['Tester'] == 'Lorenzo')].drop(columns=['Class', 'Tester']).columns\n",
    "class_col = ds[(ds['Tester'] == 'Lorenzo')]['Class']\n",
    "\n",
    "scaled_lorenzo = pd.DataFrame(scaler.fit_transform(ds[(ds['Tester'] == 'Lorenzo')].drop(columns=['Class', 'Tester'])))\n",
    "scaled_lorenzo.columns = columns\n",
    "scaled_lorenzo['Class'] = class_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var1 = X.columns[pipe_AdaBoost['feature-selection'].get_support()][0]\n",
    "var2 = X.columns[pipe_AdaBoost['feature-selection'].get_support()][1]\n",
    "\n",
    "scatter_plot(scaled_lorenzo, var1, var2, 'Lorenzo')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
